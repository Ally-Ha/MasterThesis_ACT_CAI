{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fe691f",
   "metadata": {},
   "source": [
    "# Train the Models\n",
    "\n",
    "Train 4 different models, on different datasets, based on 80/20 Split set. \n",
    "- Using PEFT (QLoRA)\n",
    "\n",
    "- following the `HuggingFace/Alignment Handbook`. \n",
    "\n",
    "- Basemodel: Llama-3.1-8B-Instruct (accessed via unsloth for efficiency). \n",
    "\n",
    "- WandB for experiment tracking and visualizations\n",
    "\n",
    "All specific configurations are found in `notebooks/src/configs.py` and `notebooks/src/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4cccd3",
   "metadata": {},
   "source": [
    "# 0. Set-up & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40478e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-y2os547u/unsloth_501bce0e07234cf7b974bc1347bb121a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-y2os547u/unsloth_501bce0e07234cf7b974bc1347bb121a\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 0eebf900ff704810ee62585ba1fa1394baabd0bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hcanceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "# Install/upgrade unsloth and pin huggingface_hub to a compatible version\n",
    "!pip install -U \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration dataclasses following alignment-handbook pattern.\n",
    "All hyperparameters follow MentalChat16K paper (Xu et al., 2025).\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Model Presets - Datasets and Output Directory\n",
    "MODEL_PRESETS = {\n",
    "    \"modelpilot\":{\n",
    "        \"dataset\": \"ShenLab/MentalChat16K\",\n",
    "        \"output_dir\": \"data/model_pilot-sft-llama-3.1-8b-instruct\",\n",
    "    },\n",
    "    \"model0\": {\n",
    "        \"dataset\": \"ShenLab/MentalChat16K\",\n",
    "        \"output_dir\": \"data/model_0-sft-llama-3.1-8b-instruct\",\n",
    "    },\n",
    "    \"model1\": {\n",
    "        \"dataset\": \"data/ds_generic\",\n",
    "        \"output_dir\": \"data/model_1-sft-llama-3.1-8b-instruct\",\n",
    "    },\n",
    "    \"model2\": {\n",
    "        \"dataset\": \"data/ds_constitution\",\n",
    "        \"output_dir\": \"data/model_2-sft-llama-3.1-8b-instruct\",\n",
    "    },\n",
    "    \"model3\": {\n",
    "        \"dataset\": \"data/ds_constitution_revised\",\n",
    "        \"output_dir\": \"data/model_3-sft-llama-3.1-8b-instruct\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Configuration Dataclasses\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration.\"\"\"\n",
    "    model_name_or_path: str = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    model_revision: str = \"main\"\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    attn_implementation: str = \"flash_attention_2\"\n",
    "    trust_remote_code: bool = False\n",
    "    max_seq_length: int = 2048\n",
    "    load_in_4bit: bool = True\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class LoraConfig:\n",
    "    \"\"\"LoRA/QLoRA configuration following MentalChat16K paper.\"\"\"\n",
    "    r: int = 64  \n",
    "    lora_alpha: int = 16  \n",
    "    lora_dropout: float = 0.1 \n",
    "    target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    bias: str = \"none\"\n",
    "    use_gradient_checkpointing: str = \"unsloth\"\n",
    "    random_state: int = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Dataset configuration.\"\"\"\n",
    "    dataset_id: str = \"ShenLab/MentalChat16K\"\n",
    "    dataset_config: Optional[str] = None\n",
    "    dataset_split: str = \"train\"\n",
    "    test_split_size: float = 0.2\n",
    "    seed: int = 42\n",
    "    num_proc: int = 4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"Data generation configuration using AZURE OpenAI API.\"\"\"\n",
    "    model: str = \"o3-mini\"\n",
    "    reasoning_level: str = \"medium\"       # \"low\" | \"medium\" | \"high\"\n",
    "    reasoning_summary: str = \"concise\"    # \"auto\" | \"concise\" | \"detailed\" | \"none\"\n",
    "    max_completion_tokens: int = 1500     # includes reasoning tokens; raise if truncated\n",
    "    # HuggingFace Hub\n",
    "    hf_username: str = \"AIforAlly\"\n",
    "    repo_generic: str = \"mentalchat16k-generic-responses\"\n",
    "    repo_constitution: str = \"mentalchat16k-constitution-responses\"\n",
    "    # Output paths\n",
    "    output_dir: str = \"data/responses/working_files/\"\n",
    "    generic_csv: str = \"response_generic.csv\"\n",
    "    constitution_csv: str = \"response_constitution.csv\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration following MentalChat16K paper.\"\"\"\n",
    "    output_dir: str = \"data/sft-output\"\n",
    "    \n",
    "    # Optimizer settings (MentalChat16K paper)\n",
    "    learning_rate: float = 2.0e-4\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 8  # Effective batch size: 64\n",
    "    num_train_epochs: int = 5\n",
    "    max_steps: int = -1\n",
    "    warmup_ratio: float = 0.03\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 0.3\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    optim: str = \"paged_adamw_32bit\"\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy: str = \"no\"  # No eval during training for reproduction\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_steps: int = 100\n",
    "    save_total_limit: int = 2\n",
    "    logging_steps: int = 10\n",
    "    \n",
    "    # Precision and efficiency\n",
    "    bf16: bool = True\n",
    "    fp16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub: bool = False\n",
    "    hub_model_id: Optional[str] = None\n",
    "    \n",
    "    # Reporting\n",
    "    report_to: List[str] = field(default_factory=lambda: [\"wandb\"])\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# Combined Configuration\n",
    "\n",
    "@dataclass\n",
    "class SFTScriptConfig:\n",
    "    \"\"\"Combined configuration for SFT script.\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    lora: LoraConfig = field(default_factory=LoraConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    generation: GenerationConfig = field(default_factory=GenerationConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    \n",
    "    @classmethod\n",
    "    def for_model(cls, model_id: str) -> \"SFTScriptConfig\":\n",
    "        \"\"\"\n",
    "        Get config for specific model experiment.\n",
    "        \n",
    "        Args:\n",
    "            model_id: One of 'model0', 'model1', 'model2', 'model3'\n",
    "        \n",
    "        Returns:\n",
    "            SFTScriptConfig with model-specific settings applied\n",
    "        \n",
    "        Example:\n",
    "            config = SFTScriptConfig.for_model(\"model0\")\n",
    "        \"\"\"\n",
    "        if model_id not in MODEL_PRESETS:\n",
    "            raise ValueError(f\"Unknown model_id: {model_id}. Choose from {list(MODEL_PRESETS.keys())}\")\n",
    "        \n",
    "        preset = MODEL_PRESETS[model_id]\n",
    "        config = cls()\n",
    "        config.data.dataset_id = preset[\"dataset\"]\n",
    "        config.training.output_dir = preset[\"output_dir\"]\n",
    "        return config\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert config to dictionary.\"\"\"\n",
    "        from dataclasses import asdict\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.26.5.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwanted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0m_compare_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.26.5.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-17148/843123145.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#         except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     raise ImportError(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"UNSLOTH_ZOO_IS_PRESENT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m from .temporary_patches import (\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mencode_conversations_with_harmony\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgemma3n\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/gemma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEMPORARY_PATCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpatch_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprocess_output_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"Unpack\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     raise RuntimeError(\n\u001b[1;32m    124\u001b[0m         \u001b[0;34mf\"Unsloth: Unpack has been moved! Other error = {str(e)}.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.26.5.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "# model.py\n",
    "\"\"\"\n",
    "Model loading with Unsloth for efficient QLoRA training.\n",
    "Following alignment-handbook pattern but using Unsloth backend.\n",
    "\"\"\"\n",
    "import logging\n",
    "import torch\n",
    "from typing import Tuple, Optional\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(\n",
    "    model_name: str,\n",
    "    max_seq_length: int = 2048,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "    load_in_4bit: bool = True,\n",
    ") -> Tuple[FastLanguageModel, any]:\n",
    "    \"\"\"\n",
    "    Load model and tokenizer using Unsloth for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model ID or Unsloth optimized model\n",
    "        max_seq_length: Maximum sequence length\n",
    "        dtype: Model dtype (None = auto-detect)\n",
    "        load_in_4bit: Whether to use 4-bit quantization\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def apply_peft(\n",
    "    model: FastLanguageModel,\n",
    "    r: int = 64,  # MentalChat16K paper\n",
    "    lora_alpha: int = 16,  # MentalChat16K paper\n",
    "    lora_dropout: float = 0.1,  # MentalChat16K paper\n",
    "    target_modules: list = None,\n",
    "    bias: str = \"none\",\n",
    "    use_gradient_checkpointing: str = \"unsloth\",\n",
    "    random_state: int = 42,\n",
    ") -> FastLanguageModel:\n",
    "    \"\"\"\n",
    "    Apply PEFT/LoRA configuration using Unsloth's optimized implementation.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model from get_model_and_tokenizer\n",
    "        r: LoRA rank\n",
    "        lora_alpha: LoRA alpha parameter\n",
    "        lora_dropout: Dropout probability\n",
    "        target_modules: List of modules to apply LoRA\n",
    "        bias: Bias setting (\"none\", \"all\", \"lora_only\")\n",
    "        use_gradient_checkpointing: Gradient checkpointing mode\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Model with PEFT applied\n",
    "    \"\"\"\n",
    "    if target_modules is None:\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ]\n",
    "    \n",
    "    logger.info(f\"Applying LoRA with r={r}, alpha={lora_alpha}\")\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        bias=bias,\n",
    "        use_gradient_checkpointing=use_gradient_checkpointing,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_for_inference(model: FastLanguageModel) -> FastLanguageModel:\n",
    "    \"\"\"\n",
    "    Prepare model for inference (2x faster generation).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "    \n",
    "    Returns:\n",
    "        Model optimized for inference\n",
    "    \"\"\"\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e687e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data loading and preprocessing following alignment-handbook pattern.\n",
    "\"\"\"\n",
    "import logging\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_and_split_dataset(\n",
    "    dataset_id: str,\n",
    "    dataset_config: Optional[str] = None,\n",
    "    dataset_split: str = \"train\",\n",
    "    test_split_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "    **kwargs\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Load and split dataset following alignment-handbook pattern.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: HuggingFace dataset ID\n",
    "        dataset_config: Dataset configuration name\n",
    "        dataset_split: Split to load\n",
    "        test_split_size: Fraction for test split (0.0 = no split, 0.2 = 20%)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with train (and optionally test) splits\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dataset: {dataset_id}\")\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "        dataset_id,\n",
    "        dataset_config,\n",
    "        split=dataset_split\n",
    "    )\n",
    "    \n",
    "    if test_split_size > 0:\n",
    "        logger.info(f\"Splitting dataset with test_size={test_split_size}\")\n",
    "        split = dataset.train_test_split(test_size=test_split_size, seed=seed)\n",
    "        return DatasetDict({\n",
    "            'train': split['train'],\n",
    "            'test': split['test']\n",
    "        })\n",
    "    \n",
    "    return DatasetDict({'train': dataset})\n",
    "\n",
    "\n",
    "def format_to_messages(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert dataset format to chat messages.\n",
    "    Expected input columns: instruction, input, output\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example.get('instruction', '')},\n",
    "        {\"role\": \"user\", \"content\": example.get('input', '')},\n",
    "        {\"role\": \"assistant\", \"content\": example.get('output', '')}\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def apply_chat_template(example: dict, tokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Apply tokenizer's chat template to messages.\n",
    "    \n",
    "    Args:\n",
    "        example: Dict with 'messages' key\n",
    "        tokenizer: HuggingFace tokenizer with chat_template\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'text' key containing formatted conversation\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset: DatasetDict, tokenizer, num_proc: int = 4) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Prepare dataset for SFT training.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Raw dataset with instruction/input/output columns\n",
    "        tokenizer: Tokenizer for chat template\n",
    "        num_proc: Number of processes for mapping\n",
    "    \n",
    "    Returns:\n",
    "        Processed dataset with 'text' column ready for SFTTrainer\n",
    "    \"\"\"\n",
    "    logger.info(\"Formatting dataset to messages...\")\n",
    "    dataset = dataset.map(format_to_messages, num_proc=num_proc)\n",
    "    \n",
    "    logger.info(\"Applying chat template...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: apply_chat_template(x, tokenizer),\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76756558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT training with Optuna hyperparameter optimization.\n",
    "Following alignment-handbook pattern with Unsloth efficiency.\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import set_seed\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def create_training_args(\n",
    "    training_cfg,  # TrainingConfig\n",
    "    max_seq_length: int,\n",
    ") -> SFTConfig:\n",
    "    \"\"\"\n",
    "    Create SFT training arguments from TrainingConfig.\n",
    "    \n",
    "    Args:\n",
    "        training_cfg: TrainingConfig dataclass instance\n",
    "        max_seq_length: From ModelConfig (needed for SFTConfig)\n",
    "    \n",
    "    Returns:\n",
    "        SFTConfig for TRL SFTTrainer\n",
    "    \"\"\"\n",
    "    return SFTConfig(\n",
    "        output_dir=training_cfg.output_dir,\n",
    "        learning_rate=training_cfg.learning_rate,\n",
    "        per_device_train_batch_size=training_cfg.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_cfg.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=training_cfg.gradient_accumulation_steps,\n",
    "        num_train_epochs=training_cfg.num_train_epochs,\n",
    "        max_seq_length=max_seq_length,\n",
    "        eval_strategy=training_cfg.eval_strategy,\n",
    "        save_strategy=training_cfg.save_strategy,\n",
    "        save_steps=training_cfg.save_steps,\n",
    "        logging_steps=training_cfg.logging_steps,\n",
    "        warmup_ratio=training_cfg.warmup_ratio,\n",
    "        weight_decay=training_cfg.weight_decay,\n",
    "        max_grad_norm=training_cfg.max_grad_norm,\n",
    "        lr_scheduler_type=training_cfg.lr_scheduler_type,\n",
    "        optim=training_cfg.optim,\n",
    "        bf16=training_cfg.bf16,\n",
    "        fp16=training_cfg.fp16,\n",
    "        gradient_checkpointing=training_cfg.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        save_total_limit=training_cfg.save_total_limit,\n",
    "        seed=training_cfg.seed,\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=False,\n",
    "        report_to=training_cfg.report_to,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_trainer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    training_args: SFTConfig,\n",
    ") -> SFTTrainer:\n",
    "    \"\"\"\n",
    "    Create SFT trainer following alignment-handbook pattern.\n",
    "    \"\"\"\n",
    "    return SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    config: SFTScriptConfig\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main training function.\n",
    "    \n",
    "    Args:\n",
    "        config: Full SFT configuration\n",
    "        trial: Optional Optuna trial for hyperparameter search\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results\n",
    "    \"\"\"\n",
    "    set_seed(config.training.seed)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = get_model_and_tokenizer(\n",
    "        model_name=config.model.model_name_or_path,\n",
    "        max_seq_length=config.model.max_seq_length,\n",
    "        load_in_4bit=config.model.load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    # Apply PEFT\n",
    "    model = apply_peft(\n",
    "        model,\n",
    "        r=config.lora.r,\n",
    "        lora_alpha=config.lora.lora_alpha,\n",
    "        lora_dropout=config.lora.lora_dropout,\n",
    "        target_modules=config.lora.target_modules,\n",
    "        bias=config.lora.bias,\n",
    "        use_gradient_checkpointing=config.lora.use_gradient_checkpointing,\n",
    "        random_state=config.lora.random_state,\n",
    "    )\n",
    "    \n",
    "    # Load and prepare dataset\n",
    "    dataset = load_and_split_dataset(\n",
    "        dataset_id=config.data.dataset_id,\n",
    "        dataset_config=config.data.dataset_config,\n",
    "        dataset_split=config.data.dataset_split,\n",
    "        test_split_size=config.data.test_split_size,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    dataset = prepare_dataset(dataset, tokenizer, num_proc=config.data.num_proc)\n",
    "    \n",
    "    # Create training arguments\n",
    "    training_args = create_training_args(\n",
    "        training_cfg=config.training,\n",
    "        max_seq_length=config.model.max_seq_length,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = create_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset.get(\"test\"),\n",
    "        training_args=training_args,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    logger.info(\"*** Starting training ***\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = {}\n",
    "    if dataset.get(\"test\") is not None:\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        metrics[\"eval_loss\"] = eval_metrics[\"eval_loss\"]\n",
    "    \n",
    "    metrics[\"train_loss\"] = train_result.training_loss\n",
    "    \n",
    "    return {\n",
    "        \"trainer\": trainer,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"metrics\": metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b440e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab60eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config\n",
      "  Model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "  Max seq length: 2048\n",
      "  Load in 4-bit: True\n",
      "\n",
      "LoRA Config\n",
      "  Rank (r): 64\n",
      "  Alpha: 16\n",
      "  Dropout: 0.1\n",
      "\n",
      "Data Config\n",
      "  Dataset: ShenLab/MentalChat16K\n",
      "  Test split size: 0.2\n",
      "\n",
      "Training Config\n",
      "  Output dir: data/model_pilot-sft-llama-3.1-8b-instruct\n",
      "  Learning rate: 0.0002\n",
      "  Batch size: 8\n",
      "  Epochs: 5\n"
     ]
    }
   ],
   "source": [
    "config = SFTScriptConfig.for_model(\"modelpilot\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"Model Config\")\n",
    "print(f\"  Model: {config.model.model_name_or_path}\")\n",
    "print(f\"  Max seq length: {config.model.max_seq_length}\")\n",
    "print(f\"  Load in 4-bit: {config.model.load_in_4bit}\")\n",
    "\n",
    "print(\"\\nLoRA Config\")\n",
    "print(f\"  Rank (r): {config.lora.r}\")\n",
    "print(f\"  Alpha: {config.lora.lora_alpha}\")\n",
    "print(f\"  Dropout: {config.lora.lora_dropout}\")\n",
    "\n",
    "print(\"\\nData Config\")\n",
    "print(f\"  Dataset: {config.data.dataset_id}\")\n",
    "print(f\"  Test split size: {config.data.test_split_size}\")\n",
    "\n",
    "print(\"\\nTraining Config\")\n",
    "print(f\"  Output dir: {config.training.output_dir}\")\n",
    "print(f\"  Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  Batch size: {config.training.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {config.training.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: (1) Create a W&B account\n",
      "wandb: (2) Use an existing W&B account\n",
      "wandb: (3) Don't visualize my results\n",
      "wandb: Enter your choice:wandb: You chose 'Use an existing W&B account'\n",
      "wandb: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
      "wandb: Store your API key securely and do not share it.\n",
      "wandb: Paste your API key and hit enter:wandb: ERROR Invalid API key: API key must have 40+ characters, has 1.\n",
      "wandb: (1) Create a W&B account\n",
      "wandb: (2) Use an existing W&B account\n",
      "wandb: (3) Don't visualize my results\n",
      "wandb: Enter your choice:wandb: You chose 'Use an existing W&B account'\n",
      "wandb: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
      "wandb: Store your API key securely and do not share it.\n",
      "wandb: Paste your API key and hit enter:wandb: No netrc file found, creating one.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "wandb: Currently logged in as: alha8035 (alha8035-stockholm-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260226_155925-vnifsmu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/vnifsmu1' target=\"_blank\">lemon-wind-5</a></strong> to <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft' target=\"_blank\">https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/vnifsmu1' target=\"_blank\">https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/vnifsmu1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Detected [huggingface_hub.inference, openai] in use.\n",
      "wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "&lt;wandb.sdk.wandb_run.Run object at 0x7be4b267ebd0&gt;"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7be4b267ebd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Initialize run\n",
    "wandb.init(\n",
    "    entity=\"alha8035-stockholm-university\",\n",
    "    project=\"pilot_model0_sft\",\n",
    "    config=config.to_dict(),\n",
    "    tags=[\"sft\", \"qlora\", \"unsloth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d7a8c",
   "metadata": {},
   "source": [
    "# 1. Training Set-Up \n",
    "\n",
    "Following the Parameters used in the MentalChat16K Paper (Xu et al., 2025). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce2e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'huggingface_hub.constants' has no attribute 'HF_HUB_ENABLE_HF_TRANSFER'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTracebackError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_get_statistics\u001b[0;34m(statistics, force_download)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0mtime_limited_stats_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/rl_environments.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0meffective_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"process\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                 return _run_in_subprocess(func, seconds, args, kwargs,\n\u001b[0m\u001b[1;32m    776\u001b[0m                                           start_method=start_method, kill_grace=kill_grace)\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/rl_environments.py\u001b[0m in \u001b[0;36m_run_in_subprocess\u001b[0;34m(func, seconds, args, kwargs, start_method, kill_grace)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRemoteTracebackError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name}: {msg}\\nRemote traceback:\\n{tb}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteTracebackError\u001b[0m: AttributeError: module 'huggingface_hub.constants' has no attribute 'HF_HUB_ENABLE_HF_TRANSFER'\nRemote traceback:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/unsloth_zoo/rl_environments.py\", line 668, in _child\n    res = f(*a, **(kw or {}))\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\", line 1335, in stats_check\n    snapshot_download(\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 89, in _inner_fn\n    If an input is not valid.\n       ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_snapshot_download.py\", line 326, in snapshot_download\n    if constants.HF_HUB_ENABLE_HF_TRANSFER:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: module 'huggingface_hub.constants' has no attribute 'HF_HUB_ENABLE_HF_TRANSFER'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-462/2003028051.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, tokenizer = get_model_and_tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_in_4bit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipython-input-462/843123145.py\u001b[0m in \u001b[0;36mget_model_and_tokenizer\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mload_in_8bit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, load_in_fp8, **kwargs)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[0mmodel_patcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m         \u001b[0;31m# For debugging - we use a download counter to see if environments are not breaking or if HF is down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         \u001b[0mget_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local_files_only\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36mget_statistics\u001b[0;34m(local_files_only)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m         \u001b[0mdisabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m     \u001b[0m_get_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m     \u001b[0m_get_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"repeat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     total_memory = (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_get_statistics\u001b[0;34m(statistics, force_download)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 \u001b[0;31m# Try no time limit check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m                 \u001b[0mstats_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36mstats_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstats_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_cleanup_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m                     snapshot_download(\n\u001b[0m\u001b[1;32m   1336\u001b[0m                         \u001b[0;34mf\"unslothai/{statistics}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                         \u001b[0mforce_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHFValidationError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \"\"\"\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# TODO: add an argument to opt-out validation for specific argument?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/_snapshot_download.py\u001b[0m in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_HUB_ENABLE_HF_TRANSFER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;31m# when using hf_transfer we don't want extra parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# from the one hf_transfer provides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'huggingface_hub.constants' has no attribute 'HF_HUB_ENABLE_HF_TRANSFER'"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(\n",
    "    model_name=config.model.model_name_or_path,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {config.model.model_name_or_path}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PEFT/LoRA using Unsloth\n",
    "model = apply_peft(\n",
    "    model,\n",
    "    r=config.lora.r,\n",
    "    lora_alpha=config.lora.lora_alpha,\n",
    "    lora_dropout=config.lora.lora_dropout,\n",
    "    target_modules=config.lora.target_modules,\n",
    "    bias=config.lora.bias,\n",
    "    use_gradient_checkpointing=config.lora.use_gradient_checkpointing,\n",
    "    random_state=config.lora.random_state,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb83650",
   "metadata": {},
   "source": [
    "# 2. Training\n",
    "\n",
    "Training four different models, based on different datasets \n",
    "\n",
    "    - Model 0: MentalChat16K Dataset \n",
    "\n",
    "    - Model 0: ds_generic \n",
    "\n",
    "    - Model 1: ds_act \n",
    "\n",
    "    - Model 2: ds_constitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c584b",
   "metadata": {},
   "source": [
    "## 2.0 Pilot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69991ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_split_dataset(\n",
    "    dataset_id=config.data.dataset_id,\n",
    "    dataset_config=config.data.dataset_config,\n",
    "    dataset_split=config.data.dataset_split,\n",
    "    test_split_size=config.data.test_split_size,\n",
    "    seed=config.data.seed,\n",
    ")\n",
    "\n",
    "#pilot testing, small subset\n",
    "TRAIN_SUBSET = 800\n",
    "TEST_SUBSET = 200\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(min(TRAIN_SUBSET, len(dataset[\"train\"]))))\n",
    "if \"test\" in dataset:\n",
    "    dataset[\"test\"] = dataset[\"test\"].select(range(min(TEST_SUBSET, len(dataset[\"test\"]))))\n",
    "\n",
    "# Prepare dataset (format to messages, apply chat template)\n",
    "dataset = prepare_dataset(dataset, tokenizer, num_proc=config.data.num_proc)\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset.get('test', []))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151df61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-462/71691315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m training_args = create_training_args(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_training_args' is not defined"
     ]
    }
   ],
   "source": [
    "# Colab uses fp16 instead of bf16\n",
    "config.training.bf16 = False\n",
    "config.training.fp16 = True\n",
    "\n",
    "# create_training_args takes the full TrainingConfig object\n",
    "training_args = create_training_args(\n",
    "    training_cfg=config.training,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    ")\n",
    "\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset.get(\"test\"),\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1223eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Log final metrics\n",
    "print(f\"\\nTraining Complete\")\n",
    "print(f\"Final train loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Evaluate if test set exists\n",
    "if dataset.get(\"test\") is not None:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(f\"Eval loss: {eval_metrics['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bccd78",
   "metadata": {},
   "source": [
    "## 2.1 Model 0: MentalChat16K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b13aa",
   "metadata": {},
   "source": [
    "## 2.2 Model 1: ds_generic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe6dee",
   "metadata": {},
   "source": [
    "## 2.3 Model 2: ds_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5282c",
   "metadata": {},
   "source": [
    "## 2.4 Model 3: ds_constitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f5309",
   "metadata": {},
   "source": [
    "# 3. Quick Check & Save the Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e45888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer locally\n",
    "OUTPUT_DIR = config.training.output_dir\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved locally to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save as WandB artifact\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"model_pilot-sft-llama-3.1-8b-instruct\",\n",
    "    type=\"model\",\n",
    "    description=\"Pilot SFT model fine-tuned on MentalChat16K with QLoRA\",\n",
    "    metadata=config.to_dict(),\n",
    ")\n",
    "artifact.add_dir(OUTPUT_DIR)\n",
    "wandb.log_artifact(artifact)\n",
    "print(f\"Model uploaded to WandB as artifact: model_pilot-sft-llama-3.1-8b-instruct\")\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # prepares model for 2x faster inference\n",
    "\n",
    "# Test prompt\n",
    "test_input = \"I've been feeling really anxious lately about my job. I keep thinking I'm going to get fired even though there's no evidence of that.\"\n",
    "system_prompt = \"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.  The assistant gives helpful, comprehensive, and appropriate answers to the user's questions.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": test_input}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(f\"\\nUser Input: {test_input}\")\n",
    "print(f\"\\nModel Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
