{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7bc059e",
      "metadata": {
        "id": "f7bc059e"
      },
      "source": [
        "# SFT Training Pipeline\n",
        "In addition to the HuggingFace/Alignment Handbook, the following \"tools\" were used:\n",
        "- **Unsloth** for faster training with less memory\n",
        "- **QLoRA** for parameter-efficient fine-tuning\n",
        "- **Optuna** for hyperparameter optimization\n",
        "- **WandB** for experiment tracking and visualization\n",
        "\n",
        "## Pipeline Structure\n",
        "1. Setup & Configuration\n",
        "2. Load Model & Tokenizer (Unsloth)\n",
        "3. Prepare Dataset\n",
        "4. Train Model\n",
        "5. Hyperparameter Search\n",
        "6. Save & Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up & Import\n",
        "Clone GitHub Repo for it to be run on GoogleColab"
      ],
      "metadata": {
        "id": "60gwyeujK3h6"
      },
      "id": "60gwyeujK3h6"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ally-Ha/pilot_act-cai_model0_SFT.git\n",
        "%cd pilot_act-cai_model0_SFT\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv31bYNcKYKo",
        "outputId": "e0d5d2b6-4bbd-4881-a3cd-21cf48e75cbc"
      },
      "id": "Kv31bYNcKYKo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pilot_act-cai_model0_SFT' already exists and is not an empty directory.\n",
            "/content/pilot_act-cai_model0_SFT\n",
            "Requirement already satisfied: accelerate>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes>=0.46.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.49.1)\n",
            "Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.3.0)\n",
            "Requirement already satisfied: deepspeed>=0.17.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.18.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.36.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: peft>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.18.1)\n",
            "Requirement already satisfied: safetensors>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.10.0)\n",
            "Requirement already satisfied: transformers>=4.53.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (4.57.6)\n",
            "Requirement already satisfied: trl>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.24.0)\n",
            "Requirement already satisfied: unsloth>=2024.8 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (2026.1.4)\n",
            "Requirement already satisfied: optuna>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (4.7.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (0.24.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (3.1.6)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (1.16.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (2.19.0)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 30)) (6.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.9.0->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (23.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->-r requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (1.13.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (2.12.3)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed>=0.17.2->-r requirements.txt (line 5)) (13.590.48)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.4->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.4->-r requirements.txt (line 6)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (3.6.1)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.6.0->-r requirements.txt (line 12)) (1.3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.53.3->-r requirements.txt (line 13)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.53.3->-r requirements.txt (line 13)) (0.22.2)\n",
            "Requirement already satisfied: unsloth_zoo>=2026.1.4 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (2026.1.4)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.46.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.25.0)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (1.0.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (5.29.5)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.0.34)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.36.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.5.0->-r requirements.txt (line 20)) (1.18.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna>=3.5.0->-r requirements.txt (line 20)) (6.10.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.5.0->-r requirements.txt (line 20)) (2.0.46)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 23)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 23)) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 23)) (4.5.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 23)) (2.51.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.0.0->-r requirements.txt (line 26)) (3.0.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (3.10.1)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 29)) (3.1.5)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.5.0->-r requirements.txt (line 20)) (1.3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (3.13.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 23)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->-r requirements.txt (line 4)) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->-r requirements.txt (line 4)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->-r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=4.0.0->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=4.0.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed>=0.17.2->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed>=0.17.2->-r requirements.txt (line 5)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed>=0.17.2->-r requirements.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->-r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.5.0->-r requirements.txt (line 20)) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6.0->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.4->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.15.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.4->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.4->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (11.3.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.4->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.20.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (8.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->-r requirements.txt (line 4)) (2025.3)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (4.4.4)\n",
            "\u001b[33mWARNING: unsloth 2026.1.4 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->-r requirements.txt (line 4)) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 23)) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth>=2024.8->unsloth[colab-new]>=2024.8->-r requirements.txt (line 17)) (3.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c291232",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c291232",
        "outputId": "5391c1bb-a61e-43b3-c306-c2733f684762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from src import (\n",
        "    SFTScriptConfig,\n",
        "    get_model_and_tokenizer,\n",
        "    apply_peft,\n",
        "    load_and_split_dataset,\n",
        "    prepare_dataset,\n",
        "    create_training_args,\n",
        "    create_trainer,\n",
        "    train,\n",
        "    run_hpo,\n",
        "    prepare_for_inference,\n",
        ")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1eaac5",
      "metadata": {
        "id": "5e1eaac5"
      },
      "source": [
        "## 1. Configuration\n",
        "The config follows alignment-handbook's structure with sections for: model, lora, data, training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "28a7766d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28a7766d",
        "outputId": "e5a07e48-faea-443f-fd4e-e85dda33e487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Config\n",
            "  Model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
            "  Max seq length: 2048\n",
            "  Load in 4-bit: True\n",
            "\n",
            "LoRA Config\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.05\n",
            "\n",
            "Data Config\n",
            "  Dataset: ShenLab/MentalHealth16K\n",
            "  Test split size: 1000\n",
            "\n",
            "Training Config\n",
            "  Output dir: data/llama-3.1-8b-instruct-sft-pilot\n",
            "  Learning rate: 2e-05\n",
            "  Batch size: 4\n",
            "  Epochs: 1\n"
          ]
        }
      ],
      "source": [
        "CONFIG_PATH = \"recipes/SFT/config_pilot.yaml\"\n",
        "config = SFTScriptConfig.from_yaml(CONFIG_PATH)\n",
        "\n",
        "# Display configuration\n",
        "print(\"Model Config\")\n",
        "print(f\"  Model: {config.model.model_name_or_path}\")\n",
        "print(f\"  Max seq length: {config.model.max_seq_length}\")\n",
        "print(f\"  Load in 4-bit: {config.model.load_in_4bit}\")\n",
        "\n",
        "print(\"\\nLoRA Config\")\n",
        "print(f\"  Rank (r): {config.lora.r}\")\n",
        "print(f\"  Alpha: {config.lora.lora_alpha}\")\n",
        "print(f\"  Dropout: {config.lora.lora_dropout}\")\n",
        "\n",
        "print(\"\\nData Config\")\n",
        "print(f\"  Dataset: {config.data.dataset_id}\")\n",
        "print(f\"  Test split size: {config.data.test_split_size}\")\n",
        "\n",
        "print(\"\\nTraining Config\")\n",
        "print(f\"  Output dir: {config.training.output_dir}\")\n",
        "print(f\"  Learning rate: {config.training.learning_rate}\")\n",
        "print(f\"  Batch size: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"  Epochs: {config.training.num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7579aece",
      "metadata": {
        "id": "7579aece"
      },
      "source": [
        "## 2. Initialize WandB for Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b93b69f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "b93b69f8",
        "outputId": "0ac9588b-6aaf-4aa3-8bc1-15f3e3e0ed96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.\n",
            "wandb: Currently logged in as: alha8035 (alha8035-stockholm-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/pilot_act-cai_model0_SFT/wandb/run-20260205_095455-x26cdy88</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/x26cdy88' target=\"_blank\">fancy-dream-2</a></strong> to <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft' target=\"_blank\">https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/x26cdy88' target=\"_blank\">https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/x26cdy88</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Detected [huggingface_hub.inference, openai] in use.\n",
            "wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alha8035-stockholm-university/pilot_model0_sft/runs/x26cdy88?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c29b2da7f50>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# Initialize run\n",
        "wandb.init(\n",
        "    entity=\"alha8035-stockholm-university\",\n",
        "    project=\"pilot_model0_sft\",\n",
        "    config=config.to_dict(),\n",
        "    tags=[\"sft\", \"qlora\", \"unsloth\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Model & Tokenizer (with Unsloth)"
      ],
      "metadata": {
        "id": "pikhPx_tKvve"
      },
      "id": "pikhPx_tKvve"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9a4320e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9a4320e",
        "outputId": "80f74813-5c80-4575-b1b4-bae8415cdcf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model loaded: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
            "Model dtype: torch.float16\n",
            "Tokenizer vocab size: 128256\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = get_model_and_tokenizer(\n",
        "    model_name=config.model.model_name_or_path,\n",
        "    max_seq_length=config.model.max_seq_length,\n",
        "    load_in_4bit=config.model.load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {config.model.model_name_or_path}\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d8da8477",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8da8477",
        "outputId": "a5b71370-22e1-44b6-8dda-81f5f172650c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2026.1.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 41,943,040 (0.92%)\n"
          ]
        }
      ],
      "source": [
        "#Apply PEFT/LoRA using Unsloth\n",
        "model = apply_peft(\n",
        "    model,\n",
        "    r=config.lora.r,\n",
        "    lora_alpha=config.lora.lora_alpha,\n",
        "    lora_dropout=config.lora.lora_dropout,\n",
        "    target_modules=config.lora.target_modules,\n",
        "    bias=config.lora.bias,\n",
        "    use_gradient_checkpointing=config.lora.use_gradient_checkpointing,\n",
        "    random_state=config.lora.random_state,\n",
        ")\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc539698",
      "metadata": {
        "id": "cc539698"
      },
      "source": [
        "## 4. Prepare Dataset\n",
        "\n",
        "Load and preprocess the dataset following alignment-handbook's data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d84c6bbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "d84c6bbe",
        "outputId": "1c67aaf9-be38-40c7-a45b-7270296d92fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'ShenLab/MentalHealth16K' doesn't exist on the Hub or cannot be accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2578931691.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = load_and_split_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdataset_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdataset_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_split_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_split_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pilot_act-cai_model0_SFT/src/data.py\u001b[0m in \u001b[0;36mload_and_split_dataset\u001b[0;34m(dataset_id, dataset_config, dataset_split, test_split_size, seed, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading dataset: {dataset_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     dataset = load_dataset(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdataset_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1398\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                     raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 ) from e\n\u001b[1;32m    984\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 api.hf_hub_download(\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'ShenLab/MentalHealth16K' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ],
      "source": [
        "dataset = load_and_split_dataset(\n",
        "    dataset_id=config.data.dataset_id,\n",
        "    dataset_config=config.data.dataset_config,\n",
        "    dataset_split=config.data.dataset_split,\n",
        "    test_split_size=config.data.test_split_size,\n",
        "    seed=config.data.seed,\n",
        ")\n",
        "\n",
        "# Prepare dataset (format to messages, apply chat template)\n",
        "dataset = prepare_dataset(dataset, tokenizer, num_proc=config.data.num_proc)\n",
        "\n",
        "print(f\"Train samples: {len(dataset['train'])}\")\n",
        "print(f\"Test samples: {len(dataset.get('test', []))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train Model\n",
        "\n",
        "Create trainer and run training following alignment-handbook's training loop."
      ],
      "metadata": {
        "id": "jhVwAE2uLF_9"
      },
      "id": "jhVwAE2uLF_9"
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = create_training_args(\n",
        "    output_dir=config.training.output_dir,\n",
        "    learning_rate=config.training.learning_rate,\n",
        "    per_device_train_batch_size=config.training.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
        "    num_train_epochs=config.training.num_train_epochs,\n",
        "    max_seq_length=config.model.max_seq_length,\n",
        "    eval_strategy=config.training.eval_strategy,\n",
        "    eval_steps=config.training.eval_steps,\n",
        "    save_steps=config.training.save_steps,\n",
        "    logging_steps=config.training.logging_steps,\n",
        "    warmup_ratio=config.training.warmup_ratio,\n",
        "    weight_decay=config.training.weight_decay,\n",
        "    lr_scheduler_type=config.training.lr_scheduler_type,\n",
        "    optim=config.training.optim,\n",
        "    bf16=config.training.bf16,\n",
        "    gradient_checkpointing=config.training.gradient_checkpointing,\n",
        "    save_total_limit=config.training.save_total_limit,\n",
        "    seed=config.training.seed,\n",
        "    report_to=config.training.report_to,\n",
        ")\n",
        "\n",
        "trainer = create_trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset.get(\"test\"),\n",
        "    training_args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer created successfully!\")"
      ],
      "metadata": {
        "id": "Mz1f2W-VLLHj"
      },
      "id": "Mz1f2W-VLLHj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf6e52c",
      "metadata": {
        "id": "ccf6e52c"
      },
      "outputs": [],
      "source": [
        "# Run Training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Log final metrics\n",
        "print(f\"\\n=== Training Complete ===\")\n",
        "print(f\"Final train loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "# Evaluate if test set exists\n",
        "if dataset.get(\"test\") is not None:\n",
        "    eval_metrics = trainer.evaluate()\n",
        "    print(f\"Eval loss: {eval_metrics['eval_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6bdc3a",
      "metadata": {
        "id": "5c6bdc3a"
      },
      "source": [
        "## 6. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a86b9fa",
      "metadata": {
        "id": "9a86b9fa"
      },
      "outputs": [],
      "source": [
        "# Save Model and Tokenizer\n",
        "OUTPUT_DIR = config.training.output_dir\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23510e31",
      "metadata": {
        "id": "23510e31"
      },
      "source": [
        "## 7. Hyperparameter Optimization with Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "502465a7",
      "metadata": {
        "id": "502465a7"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.visualization import plot_param_importances, plot_optimization_history\n",
        "import plotly\n",
        "\n",
        "# Reload config for HPO (starts fresh)\n",
        "hpo_config = SFTScriptConfig.from_yaml(CONFIG_PATH)\n",
        "\n",
        "# Run HPO study\n",
        "study = run_hpo(\n",
        "    config=hpo_config,\n",
        "    n_trials=20,  # Adjust based on your compute budget\n",
        "    study_name=\"pilot_sft_hpo\",\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n=== Best Hyperparameters ===\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nBest eval loss: {study.best_value:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "fig = plot_param_importances(study)\n",
        "fig.show()\n",
        "fig = plot_optimization_history(study)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3e1991",
      "metadata": {
        "id": "1e3e1991"
      },
      "source": [
        "## 8. Quick Inference Test\n",
        "\n",
        "Test the trained model with a sample prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46f0443",
      "metadata": {
        "id": "c46f0443"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompt\n",
        "test_input = \"I've been feeling really anxious lately about my job. I keep thinking I'm going to get fired even though there's no evidence of that.\"\n",
        "system_prompt = \"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.  The assistant gives helpful, comprehensive, and appropriate answers to the user's questions.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": test_input}\n",
        "]\n",
        "\n",
        "# Apply chat template\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Generate response\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "print(f\"\\nUser Input: {test_input}\")\n",
        "print(f\"\\nModel Response: {response}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}