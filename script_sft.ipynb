{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc059e",
   "metadata": {},
   "source": [
    "# SFT Training Pipeline\n",
    "In addition to the HuggingFace/Alignment Handbook, the following \"tools\" were used: \n",
    "- **Unsloth** for faster training with less memory\n",
    "- **QLoRA** for parameter-efficient fine-tuning\n",
    "- **Optuna** for hyperparameter optimization \n",
    "- **WandB** for experiment tracking and visualization\n",
    "\n",
    "## Pipeline Structure\n",
    "1. Setup & Configuration\n",
    "2. Load Model & Tokenizer (Unsloth)\n",
    "3. Prepare Dataset\n",
    "4. Train Model\n",
    "5. Hyperparameter Search\n",
    "6. Save & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c291232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import local modules\n",
    "from src import (\n",
    "    SFTScriptConfig,\n",
    "    get_model_and_tokenizer,\n",
    "    apply_peft,\n",
    "    load_and_split_dataset,\n",
    "    prepare_dataset,\n",
    "    create_training_args,\n",
    "    create_trainer,\n",
    "    train,\n",
    "    run_hpo,\n",
    "    prepare_for_inference,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1eaac5",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "The config follows alignment-handbook's structure with sections for: model, lora, data, training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Configuration from YAML\n",
    "CONFIG_PATH = \"recipes/SFT/config_pilot.yaml\"\n",
    "config = SFTScriptConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Model Config ===\")\n",
    "print(f\"  Model: {config.model.model_name_or_path}\")\n",
    "print(f\"  Max seq length: {config.model.max_seq_length}\")\n",
    "print(f\"  Load in 4-bit: {config.model.load_in_4bit}\")\n",
    "\n",
    "print(\"\\n=== LoRA Config ===\")\n",
    "print(f\"  Rank (r): {config.lora.r}\")\n",
    "print(f\"  Alpha: {config.lora.lora_alpha}\")\n",
    "print(f\"  Dropout: {config.lora.lora_dropout}\")\n",
    "\n",
    "print(\"\\n=== Data Config ===\")\n",
    "print(f\"  Dataset: {config.data.dataset_id}\")\n",
    "print(f\"  Test split size: {config.data.test_split_size}\")\n",
    "\n",
    "print(\"\\n=== Training Config ===\")\n",
    "print(f\"  Output dir: {config.training.output_dir}\")\n",
    "print(f\"  Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  Batch size: {config.training.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {config.training.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579aece",
   "metadata": {},
   "source": [
    "## 2. Initialize WandB for Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Initialize run\n",
    "wandb.init(\n",
    "    entity=\"alha8035-stockholm-university\",\n",
    "    project=\"pilot_model0_sft\",\n",
    "    config=config.to_dict(),\n",
    "    tags=[\"sft\", \"qlora\", \"unsloth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee74a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Load Model & Tokenizer (with Unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Model and Tokenizer with Unsloth\n",
    "model, tokenizer = get_model_and_tokenizer(\n",
    "    model_name=config.model.model_name_or_path,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {config.model.model_name_or_path}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Apply PEFT/LoRA using Unsloth's optimized implementation\n",
    "model = apply_peft(\n",
    "    model,\n",
    "    r=config.lora.r,\n",
    "    lora_alpha=config.lora.lora_alpha,\n",
    "    lora_dropout=config.lora.lora_dropout,\n",
    "    target_modules=config.lora.target_modules,\n",
    "    bias=config.lora.bias,\n",
    "    use_gradient_checkpointing=config.lora.use_gradient_checkpointing,\n",
    "    random_state=config.lora.random_state,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc539698",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset\n",
    "\n",
    "Load and preprocess the dataset following alignment-handbook's data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load and Prepare Dataset\n",
    "dataset = load_and_split_dataset(\n",
    "    dataset_id=config.data.dataset_id,\n",
    "    dataset_config=config.data.dataset_config,\n",
    "    dataset_split=config.data.dataset_split,\n",
    "    test_split_size=config.data.test_split_size,\n",
    "    seed=config.data.seed,\n",
    ")\n",
    "\n",
    "# Prepare dataset (format to messages, apply chat template)\n",
    "dataset = prepare_dataset(dataset, tokenizer, num_proc=config.data.num_proc)\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset.get('test', []))}\")\n",
    "print(f\"\\nSample text preview (truncated):\")\n",
    "print(dataset['train'][0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a494a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Train Model\n",
    "\n",
    "Create trainer and run training following alignment-handbook's training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc843c",
   "metadata": {},
   "source": [
    "# Cell 7: Create Training Arguments and Trainer\n",
    "training_args = create_training_args(\n",
    "    output_dir=config.training.output_dir,\n",
    "    learning_rate=config.training.learning_rate,\n",
    "    per_device_train_batch_size=config.training.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.training.num_train_epochs,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    eval_strategy=config.training.eval_strategy,\n",
    "    eval_steps=config.training.eval_steps,\n",
    "    save_steps=config.training.save_steps,\n",
    "    logging_steps=config.training.logging_steps,\n",
    "    warmup_ratio=config.training.warmup_ratio,\n",
    "    weight_decay=config.training.weight_decay,\n",
    "    lr_scheduler_type=config.training.lr_scheduler_type,\n",
    "    optim=config.training.optim,\n",
    "    bf16=config.training.bf16,\n",
    "    gradient_checkpointing=config.training.gradient_checkpointing,\n",
    "    save_total_limit=config.training.save_total_limit,\n",
    "    seed=config.training.seed,\n",
    "    report_to=config.training.report_to,\n",
    ")\n",
    "\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset.get(\"test\"),\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run Training\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Log final metrics\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Final train loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Evaluate if test set exists\n",
    "if dataset.get(\"test\") is not None:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(f\"Eval loss: {eval_metrics['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bdc3a",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Model and Tokenizer\n",
    "OUTPUT_DIR = config.training.output_dir\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23510e31",
   "metadata": {},
   "source": [
    "## 7. (Optional) Hyperparameter Optimization with Optuna\n",
    "\n",
    "Optuna provides smarter Bayesian optimization with trial pruning. Combined with WandB logging, you get:\n",
    "- **Optuna**: Efficient search, early pruning of bad trials\n",
    "- **WandB**: Visualization, comparison, collaboration\n",
    "\n",
    "This is generally superior to WandB Sweeps alone for finding optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502465a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run Optuna HPO (Optional - uncomment to run)\n",
    "# This will search for optimal hyperparameters across multiple trials\n",
    "\n",
    "# import optuna\n",
    "# from optuna.visualization import plot_param_importances, plot_optimization_history\n",
    "\n",
    "# # Reload config for HPO (starts fresh)\n",
    "# hpo_config = SFTScriptConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# # Run HPO study\n",
    "# study = run_hpo(\n",
    "#     config=hpo_config,\n",
    "#     n_trials=20,  # Adjust based on your compute budget\n",
    "#     study_name=\"pilot_sft_hpo\",\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# print(f\"\\n=== Best Hyperparameters ===\")\n",
    "# for key, value in study.best_params.items():\n",
    "#     print(f\"  {key}: {value}\")\n",
    "# print(f\"\\nBest eval loss: {study.best_value:.4f}\")\n",
    "\n",
    "# # Visualize (requires plotly)\n",
    "# # fig = plot_param_importances(study)\n",
    "# # fig.show()\n",
    "# # fig = plot_optimization_history(study)\n",
    "# # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e1991",
   "metadata": {},
   "source": [
    "## 8. Quick Inference Test\n",
    "\n",
    "Test the trained model with a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Quick Inference Test\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Prepare model for inference (2x faster)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_input = \"I've been feeling really anxious lately about my job. I keep thinking I'm going to get fired even though there's no evidence of that.\"\n",
    "system_prompt = \"You are a helpful mental health counselling assistant. Please provide supportive and appropriate responses to the user's concerns.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": test_input}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and display\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(\"=== User Input ===\")\n",
    "print(test_input)\n",
    "print(\"\\n=== Model Response ===\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
